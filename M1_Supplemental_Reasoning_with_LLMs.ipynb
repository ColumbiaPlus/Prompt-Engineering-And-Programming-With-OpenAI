{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ColumbiaPlus/Prompt-Engineering-And-Programming-With-OpenAI/blob/main/M1_Supplemental_Reasoning_with_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hi everyone! We've been working with LLMs for a while and we all saw their great performance in remembering olympic records and writing code. However, have you wondering if LLMs can think and if they can, what's the thought process like?\n",
        "\n",
        "# Chain of thought\n",
        "\n",
        "Introduced in [Wei et al. (2022)](https://arxiv.org/abs/2201.11903), chain-of-thought (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding.\n",
        "\n",
        "![CoT_img](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcot.1933d9fe.png&w=1080&q=75)\n",
        "\n",
        "Image Source: [Wei et al. (2022)](https://arxiv.org/abs/2201.11903)"
      ],
      "metadata": {
        "id": "vz2p_EHpZFSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reasoning models\n",
        "\n",
        "Reasoning models, like OpenAI o1/o3-mini and DeepSeek R1, are new large language models trained with reinforcement learning to perform complex reasoning. Reasoning models **think before they answer**, producing a long internal **chain of thought** before responding to the user. Reasoning models excel in complex problem solving, coding, scientific reasoning, and multi-step planning for agentic workflows.\n",
        "\n",
        "Source: https://platform.openai.com/docs/guides/reasoning\n"
      ],
      "metadata": {
        "id": "v5snqk4fNylk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How reasoning works\n",
        "\n",
        "Reasoning models introduce reasoning tokens in addition to input and output tokens. The models use these reasoning tokens to \"think\", breaking down their understanding of the prompt and considering multiple approaches to generating a response. After generating reasoning tokens(CoT), the model produces an answer and discard the reasoning tokens.\n",
        "\n",
        "![Reasoning_Example_DS](https://cdn.deepseek.com/api-docs/deepseek_r1_multiround_example_en.png)\n",
        "\n",
        "Image Source: [DeepSeek](https://api-docs.deepseek.com/guides/reasoning_model#multi-round-conversation)\n"
      ],
      "metadata": {
        "id": "_gFaESWeN_ii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment: We'll see if reasoning models can replace me and design this course all by itself"
      ],
      "metadata": {
        "id": "AQJmXgk02w9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Help me plan a 15 minute online course on reasoning LLMs such as DeepSeek R1 and OpenAI o1. \\\n",
        "My target audiences are technical professionals seeking to develop AI skills. \\\n",
        "Use the related knowledge: \""
      ],
      "metadata": {
        "id": "iYQTrtDdw7p1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount the google drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "pth = '/content/drive/MyDrive/Colab Notebooks'\n",
        "os.chdir(pth)\n",
        "\n",
        "# load the knowledge\n",
        "with open(\"knowledge.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    knowledge_text = f.read()"
      ],
      "metadata": {
        "id": "UF0U5x1yMcgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(knowledge_text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYu6eQj4_j9s",
        "outputId": "d9d9ba73-fd7e-479c-eaa4-158d85d0f02a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reasoning models\n",
            "================\n",
            "\n",
            "Explore advanced reasoning and problem-solving models.\n",
            "\n",
            "**Reasoning models**, like OpenAI o1 and o3-mini, are new large language models trained with reinforcement learning to perform complex reasoning. Reasoning models [think before they answer](https://openai.com/index/introducing-openai-o1-preview/), producing a long internal chain of thought before responding to the user. Reasoning models excel in complex problem solving, coding, scientific reasoning, and multi-step planning for agentic workflows.\n",
            "\n",
            "As with our GPT models, we provide both a smaller, faster model (`o3-mini`) that is less expensive per token, and a larger model (`o1`) that is somewhat slower and more expensive, but can often generate better responses for complex tasks, and generalize better across domains.\n",
            "\n",
            "Quickstart\n",
            "----------\n",
            "\n",
            "Reasoning models can be used through the [chat completions](/docs/api-reference/chat/create) endpoint as seen here.\n",
            "\n",
            "Using a reasoning model in chat completi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt += knowledge_text"
      ],
      "metadata": {
        "id": "vYECzT4kMow2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DeepSeek R1\n",
        "**Token Limits:**\n",
        "* Context Length: 64K. *The API supports a maximum context length of 64K, and the length of the output `reasoning_content` (CoT) is not counted within the 64K context length.*\n",
        "* Max CoT Token: 32K. *Billed as output token*\n",
        "* Max Output Token: 8K. *If `max_tokens` is not specified, the default maximum output length is 4K. Adjust `max_tokens` to support longer outputs.*\n",
        "\n",
        "**Feature:**\n",
        "* API provides users with access to the CoT content generated by deepseek-reasoner\n",
        "* Can view, display, and distill the CoT content.\n",
        "* Affordable pricing (At least for now):\n",
        "    * ðŸ’° $0.14 / million input tokens (cache hit)\n",
        "    * ðŸ’° $0.55 / million input tokens (cache miss)\n",
        "    * ðŸ’° $2.19 / million output tokens\n",
        "    * ðŸ’° 75% off at off-peak hours (16:30-00:30 UTC daily).\n",
        "\n",
        "**API Documentation:**\n",
        "* Official Documentation: https://api-docs.deepseek.com/\n",
        "* LangChain Integration: https://python.langchain.com/docs/integrations/chat/deepseek/\n",
        "\n"
      ],
      "metadata": {
        "id": "JzLxgUC3ufah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install neccessary package\n",
        "!pip install openai langchain-deepseek langchain-openai --quiet\n",
        "\n",
        "# Read the key from the file and set the environmental variable\n",
        "with open(\"deepseek\",\"r\") as f:\n",
        "  deepseek_key=f.read().strip()\n",
        "os.environ[\"DEEPSEEK_API_KEY\"] = deepseek_key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqaUgB-lcL15",
        "outputId": "09b8b525-5f55-4513-d6c4-a18127bc5ddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "deepseek_client = OpenAI(api_key=deepseek_key, base_url=\"https://api.deepseek.com\")"
      ],
      "metadata": {
        "id": "tDq8jAERw6yD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reasoner_response = deepseek_client.chat.completions.create(\n",
        "    model=\"deepseek-reasoner\",\n",
        "    # reasoning_effort = medium, # a future feature that controls the length of CoT\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        ")\n",
        "\n",
        "# parse the chatcompletion object manually\n",
        "reasoner_usage = reasoner_response.usage\n",
        "CoT_content = reasoner_response.choices[0].message.reasoning_content\n",
        "reasoner_answer = reasoner_response.choices[0].message.content"
      ],
      "metadata": {
        "id": "RavpKO_Gx3Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reasoner_usage"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QgkoNDb3hkm",
        "outputId": "92561dc5-f43a-4ec5-e545-3c77728d2e66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletionUsage(completion_tokens=1668, prompt_tokens=27087, total_tokens=28755, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=874, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0), prompt_cache_hit_tokens=0, prompt_cache_miss_tokens=27087)"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(CoT_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4XOqPzqzYsz",
        "outputId": "56e37568-1905-4166-8c09-ced64f2f730b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, I need to help plan a 15-minute online course on reasoning LLMs like DeepSeek R1 and OpenAI o1 for technical professionals. The course should include coding examples in Python. Let me start by breaking down the content provided.\n",
            "\n",
            "First, the user provided documentation on both OpenAI's reasoning models (o1 and o3-mini) and DeepSeek's R1. The key points are:\n",
            "\n",
            "- Reasoning models use internal chains of thought before responding.\n",
            "- Parameters like `reasoning_effort` control the depth of thinking.\n",
            "- They excel in coding, STEM, planning, and complex problem-solving.\n",
            "- Coding examples in JavaScript, Python, and curl are given for tasks like matrix transposition, React component refactoring, and project planning.\n",
            "- DeepSeek's model also uses CoT (Chain of Thought) and provides `reasoning_content` in the API response.\n",
            "\n",
            "The target audience is technical professionals looking to develop AI skills, so the course needs to be concise but informative, with hands-on coding examples.\n",
            "\n",
            "Structure ideas:\n",
            "\n",
            "1. **Introduction (2 mins):** What are Reasoning LLMs? Differentiate from standard LLMs. Mention OpenAI o1 and DeepSeek R1.\n",
            "\n",
            "2. **Key Features (3 mins):** Chain of Thought, `reasoning_effort` parameter, cost management with `max_completion_tokens`.\n",
            "\n",
            "3. **Coding Examples (7 mins):** \n",
            "   - Use Python to show API calls.\n",
            "   - Example 1: Matrix transpose script (similar to the bash example but in Python).\n",
            "   - Example 2: React component refactoring (Python API call).\n",
            "   - Example 3: Project planning (Python API call).\n",
            "\n",
            "4. **DeepSeek R1 Specifics (2 mins):** Accessing `reasoning_content`, multi-turn conversations.\n",
            "\n",
            "5. **Best Practices & Conclusion (1 min):** When to use reasoning models, managing tokens, further resources.\n",
            "\n",
            "Now, need to ensure that each section flows within the time limit. Coding examples should be simple but demonstrate key parameters. Maybe adapt the existing JavaScript examples to Python, as the user requested Python.\n",
            "\n",
            "Potential issues:\n",
            "\n",
            "- Time constraints. 15 minutes is tight, so examples need to be concise.\n",
            "- Ensuring code examples are clear and executable. Maybe use placeholders for API keys.\n",
            "- Differentiating between OpenAI and DeepSeek APIs, but since the user wants to cover both, need to highlight their similarities and differences.\n",
            "\n",
            "Possible adjustments:\n",
            "\n",
            "- Focus on OpenAI examples first, then briefly touch on DeepSeek's `reasoning_content`.\n",
            "- Use comments in code to explain parameters.\n",
            "- Provide code snippets that users can run, maybe with a provided API key or instructing them to use their own.\n",
            "\n",
            "Final outline:\n",
            "\n",
            "**1. Introduction (2 mins)**\n",
            "- Define Reasoning LLMs: models that generate internal chains of thought.\n",
            "- Use cases: complex problem-solving, coding, planning.\n",
            "- Models: OpenAI o1/o3-mini, DeepSeek R1.\n",
            "\n",
            "**2. Key Parameters and Concepts (3 mins)**\n",
            "- `reasoning_effort`: low, medium, high.\n",
            "- `max_completion_tokens` for cost control.\n",
            "- Context window management.\n",
            "\n",
            "**3. Python Coding with OpenAI (7 mins)**\n",
            "- Install OpenAI client: `pip install openai`.\n",
            "- Example 1: Matrix transpose script.\n",
            "  - Show prompt, API call, explain `reasoning_effort`.\n",
            "- Example 2: React refactoring.\n",
            "  - Emphasize clear instructions in the prompt.\n",
            "- Example 3: Project planning.\n",
            "  - Multi-step planning, parsing the response.\n",
            "\n",
            "**4. DeepSeek R1 Example (2 mins)**\n",
            "- Install DeepSeek client: `pip install openai` (same package).\n",
            "- Show how to access `reasoning_content`.\n",
            "- Multi-turn conversation example.\n",
            "\n",
            "**5. Best Practices & Closing (1 min)**\n",
            "- Use `medium` effort initially, adjust based on needs.\n",
            "- Monitor token usage.\n",
            "- Resources: OpenAI and DeepSeek documentation.\n",
            "\n",
            "Now, draft the code examples, ensuring they're Pythonic and include necessary parameters. For instance, the matrix transpose example in Python. Also, note the differences in API endpoints and parameters between OpenAI and DeepSeek.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DeepSeek R1 CoT\n",
        "Okay, I need to help plan a 15-minute online course on reasoning LLMs like DeepSeek R1 and OpenAI o1 for technical professionals. The course should include coding examples in Python. Let me start by breaking down the content provided.\n",
        "\n",
        "First, the user provided documentation on both OpenAI's reasoning models (o1 and o3-mini) and DeepSeek's R1. The key points are:\n",
        "\n",
        "- Reasoning models use internal chains of thought before responding.\n",
        "- Parameters like `reasoning_effort` control the depth of thinking.\n",
        "- They excel in coding, STEM, planning, and complex problem-solving.\n",
        "- Coding examples in JavaScript, Python, and curl are given for tasks like matrix transposition, React component refactoring, and project planning.\n",
        "- DeepSeek's model also uses CoT (Chain of Thought) and provides `reasoning_content` in the API response.\n",
        "\n",
        "The target audience is technical professionals looking to develop AI skills, so the course needs to be concise but informative, with hands-on coding examples.\n",
        "\n",
        "Structure ideas:\n",
        "\n",
        "1. **Introduction (2 mins):** What are Reasoning LLMs? Differentiate from standard LLMs. Mention OpenAI o1 and DeepSeek R1.\n",
        "\n",
        "2. **Key Features (3 mins):** Chain of Thought, `reasoning_effort` parameter, cost management with `max_completion_tokens`.\n",
        "\n",
        "3. **Coding Examples (7 mins):**\n",
        "   - Use Python to show API calls.\n",
        "   - Example 1: Matrix transpose script (similar to the bash example but in Python).\n",
        "   - Example 2: React component refactoring (Python API call).\n",
        "   - Example 3: Project planning (Python API call).\n",
        "\n",
        "4. **DeepSeek R1 Specifics (2 mins):** Accessing `reasoning_content`, multi-turn conversations.\n",
        "\n",
        "5. **Best Practices & Conclusion (1 min):** When to use reasoning models, managing tokens, further resources.\n",
        "\n",
        "Now, need to ensure that each section flows within the time limit. Coding examples should be simple but demonstrate key parameters. Maybe adapt the existing JavaScript examples to Python, as the user requested Python.\n",
        "\n",
        "Potential issues:\n",
        "\n",
        "- Time constraints. 15 minutes is tight, so examples need to be concise.\n",
        "- Ensuring code examples are clear and executable. Maybe use placeholders for API keys.\n",
        "- Differentiating between OpenAI and DeepSeek APIs, but since the user wants to cover both, need to highlight their similarities and differences.\n",
        "\n",
        "Possible adjustments:\n",
        "\n",
        "- Focus on OpenAI examples first, then briefly touch on DeepSeek's `reasoning_content`.\n",
        "- Use comments in code to explain parameters.\n",
        "- Provide code snippets that users can run, maybe with a provided API key or instructing them to use their own.\n",
        "\n",
        "Final outline:\n",
        "\n",
        "**1. Introduction (2 mins)**\n",
        "- Define Reasoning LLMs: models that generate internal chains of thought.\n",
        "- Use cases: complex problem-solving, coding, planning.\n",
        "- Models: OpenAI o1/o3-mini, DeepSeek R1.\n",
        "\n",
        "**2. Key Parameters and Concepts (3 mins)**\n",
        "- `reasoning_effort`: low, medium, high.\n",
        "- `max_completion_tokens` for cost control.\n",
        "- Context window management.\n",
        "\n",
        "**3. Python Coding with OpenAI (7 mins)**\n",
        "- Install OpenAI client: `pip install openai`.\n",
        "- Example 1: Matrix transpose script.\n",
        "  - Show prompt, API call, explain `reasoning_effort`.\n",
        "- Example 2: React refactoring.\n",
        "  - Emphasize clear instructions in the prompt.\n",
        "- Example 3: Project planning.\n",
        "  - Multi-step planning, parsing the response.\n",
        "\n",
        "**4. DeepSeek R1 Example (2 mins)**\n",
        "- Install DeepSeek client: `pip install openai` (same package).\n",
        "- Show how to access `reasoning_content`.\n",
        "- Multi-turn conversation example.\n",
        "\n",
        "**5. Best Practices & Closing (1 min)**\n",
        "- Use `medium` effort initially, adjust based on needs.\n",
        "- Monitor token usage.\n",
        "- Resources: OpenAI and DeepSeek documentation.\n",
        "\n",
        "Now, draft the code examples, ensuring they're Pythonic and include necessary parameters. For instance, the matrix transpose example in Python. Also, note the differences in API endpoints and parameters between OpenAI and DeepSeek."
      ],
      "metadata": {
        "id": "N_gH_GozBz85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(reasoner_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6rbCQC8pOmk",
        "outputId": "fbf7364a-044c-49da-cb0f-bb2086ce9a8d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**15-Minute Online Course Plan: Reasoning LLMs (DeepSeek R1 & OpenAI o1)**  \n",
            "*Target Audience: Technical Professionals*\n",
            "\n",
            "---\n",
            "\n",
            "### **1. Introduction to Reasoning LLMs (2 mins)**  \n",
            "- **What**: LLMs that generate internal \"chains of thought\" for complex problem-solving.  \n",
            "- **Key Models**: OpenAI o1/o3-mini (speed vs. accuracy), DeepSeek R1 (exposes reasoning steps).  \n",
            "- **Use Cases**: Coding, STEM research, multi-step planning.  \n",
            "\n",
            "---\n",
            "\n",
            "### **2. Core Concepts & Parameters (3 mins)**  \n",
            "- **`reasoning_effort`**: Controls depth of thinking (`low`, `medium`, `high`).  \n",
            "- **Cost Management**: Use `max_completion_tokens` to limit tokens (reasoning + output).  \n",
            "- **Context Window**: Reserve tokens for reasoning (e.g., 25k for complex tasks).  \n",
            "\n",
            "---\n",
            "\n",
            "### **3. Python Coding with OpenAI (7 mins)**  \n",
            "**Example 1: Solve a Coding Task**  \n",
            "*Task: Transpose a matrix formatted as a string.*  \n",
            "```python\n",
            "from openai import OpenAI\n",
            "client = OpenAI()  # Replace with your API key\n",
            "\n",
            "response = client.chat.completions.create(\n",
            "    model=\"o3-mini\",\n",
            "    reasoning_effort=\"high\",\n",
            "    messages=[\n",
            "        {\"role\": \"user\", \"content\": \"\"\"\n",
            "        Write a Python function that takes a matrix string like '[1,2],[3,4],[5,6]' \n",
            "        and returns the transpose as a string in the same format.\n",
            "        \"\"\"}\n",
            "    ]\n",
            ")\n",
            "print(response.choices[0].message.content)\n",
            "```\n",
            "**Key Takeaway**: Use `reasoning_effort=\"high\"` for logic-heavy tasks.\n",
            "\n",
            "---\n",
            "\n",
            "**Example 2: Code Refactoring**  \n",
            "*Task: Modify a React component to style nonfiction books.*  \n",
            "```python\n",
            "response = client.chat.completions.create(\n",
            "    model=\"o1\",\n",
            "    messages=[\n",
            "        {\"role\": \"user\", \"content\": \"\"\"\n",
            "        Refactor this React component to make nonfiction book titles red. \n",
            "        Return only the code, no markdown:\n",
            "        const books = [{title: 'Dune', category: 'fiction'}, ...];\n",
            "        function BookList() { ... }\n",
            "        \"\"\"}\n",
            "    ]\n",
            ")\n",
            "print(response.choices[0].message.content)\n",
            "```\n",
            "**Key Takeaway**: Be explicit in prompts (e.g., \"return only code\").\n",
            "\n",
            "---\n",
            "\n",
            "### **4. DeepSeek R1: Accessing Reasoning Steps (2 mins)**  \n",
            "```python\n",
            "from openai import OpenAI\n",
            "client = OpenAI(base_url=\"https://api.deepseek.com\", api_key=\"YOUR_KEY\")\n",
            "\n",
            "response = client.chat.completions.create(\n",
            "    model=\"deepseek-reasoner\",\n",
            "    messages=[{\"role\": \"user\", \"content\": \"Explain quantum entanglement.\"}]\n",
            ")\n",
            "\n",
            "# Access reasoning chain and final answer\n",
            "print(\"Reasoning:\", response.choices[0].message.reasoning_content)\n",
            "print(\"Answer:\", response.choices[0].message.content)\n",
            "```\n",
            "**Key Takeaway**: Use `reasoning_content` to debug/model transparency.\n",
            "\n",
            "---\n",
            "\n",
            "### **5. Best Practices & Closing (1 min)**  \n",
            "- **Prompting**: Give high-level goals, not step-by-step instructions.  \n",
            "- **Token Management**: Start with `reasoning_effort=\"medium\"`, monitor usage.  \n",
            "- **Resources**:  \n",
            "  - [OpenAI Reasoning Docs](https://openai.com/docs)  \n",
            "  - [DeepSeek API Guide](https://platform.deepseek.com/api)  \n",
            "\n",
            "**Q&A**: Encourage experimentation and share use-case-specific tips.  \n",
            "\n",
            "---\n",
            "\n",
            "**Course Materials**: Provide code snippets and a cheat sheet for parameters.  \n",
            "**Time Check**: Practice timing with a live demo of one example.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DeepSeek R1 Answer\n",
        "**15-Minute Online Course Plan: Reasoning LLMs (DeepSeek R1 & OpenAI o1)**  \n",
        "*Target Audience: Technical Professionals*\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Introduction to Reasoning LLMs (2 mins)**  \n",
        "- **What**: LLMs that generate internal \"chains of thought\" for complex problem-solving.  \n",
        "- **Key Models**: OpenAI o1/o3-mini (speed vs. accuracy), DeepSeek R1 (exposes reasoning steps).  \n",
        "- **Use Cases**: Coding, STEM research, multi-step planning.  \n",
        "\n",
        "---\n",
        "\n",
        "### **2. Core Concepts & Parameters (3 mins)**  \n",
        "- **`reasoning_effort`**: Controls depth of thinking (`low`, `medium`, `high`).  \n",
        "- **Cost Management**: Use `max_completion_tokens` to limit tokens (reasoning + output).  \n",
        "- **Context Window**: Reserve tokens for reasoning (e.g., 25k for complex tasks).  \n",
        "\n",
        "---\n",
        "\n",
        "### **3. Python Coding with OpenAI (7 mins)**  \n",
        "**Example 1: Solve a Coding Task**  \n",
        "*Task: Transpose a matrix formatted as a string.*  \n",
        "```python\n",
        "from openai import OpenAI\n",
        "client = OpenAI()  # Replace with your API key\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"o3-mini\",\n",
        "    reasoning_effort=\"high\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"\"\"\n",
        "        Write a Python function that takes a matrix string like '[1,2],[3,4],[5,6]'\n",
        "        and returns the transpose as a string in the same format.\n",
        "        \"\"\"}\n",
        "    ]\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "```\n",
        "**Key Takeaway**: Use `reasoning_effort=\"high\"` for logic-heavy tasks.\n",
        "\n",
        "---\n",
        "\n",
        "**Example 2: Code Refactoring**  \n",
        "*Task: Modify a React component to style nonfiction books.*  \n",
        "```python\n",
        "response = client.chat.completions.create(\n",
        "    model=\"o1\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"\"\"\n",
        "        Refactor this React component to make nonfiction book titles red.\n",
        "        Return only the code, no markdown:\n",
        "        const books = [{title: 'Dune', category: 'fiction'}, ...];\n",
        "        function BookList() { ... }\n",
        "        \"\"\"}\n",
        "    ]\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "```\n",
        "**Key Takeaway**: Be explicit in prompts (e.g., \"return only code\").\n",
        "\n",
        "---\n",
        "\n",
        "### **4. DeepSeek R1: Accessing Reasoning Steps (2 mins)**  \n",
        "```python\n",
        "from openai import OpenAI\n",
        "client = OpenAI(base_url=\"https://api.deepseek.com\", api_key=\"YOUR_KEY\")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"deepseek-reasoner\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Explain quantum entanglement.\"}]\n",
        ")\n",
        "\n",
        "# Access reasoning chain and final answer\n",
        "print(\"Reasoning:\", response.choices[0].message.reasoning_content)\n",
        "print(\"Answer:\", response.choices[0].message.content)\n",
        "```\n",
        "**Key Takeaway**: Use `reasoning_content` to debug/model transparency.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Best Practices & Closing (1 min)**  \n",
        "- **Prompting**: Give high-level goals, not step-by-step instructions.  \n",
        "- **Token Management**: Start with `reasoning_effort=\"medium\"`, monitor usage.  \n",
        "- **Resources**:  \n",
        "  - [OpenAI Reasoning Docs](https://openai.com/docs)  \n",
        "  - [DeepSeek API Guide](https://platform.deepseek.com/api)  \n",
        "\n",
        "**Q&A**: Encourage experimentation and share use-case-specific tips.  \n",
        "\n",
        "---\n",
        "\n",
        "**Course Materials**: Provide code snippets and a cheat sheet for parameters.  \n",
        "**Time Check**: Practice timing with a live demo of one example."
      ],
      "metadata": {
        "id": "DEJkBrRrCITc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0HVqS_xNCz4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI o1/o1-mini/o3-mini\n",
        "\n",
        "**Token Limits:**\n",
        "* Context Length: 200K for o1/o3-mini, 128K for o1-mini.\n",
        "* Max Output Token: 100K for o1/o3-mini, ~65k for o1-mini.\n",
        "* Details: https://platform.openai.com/docs/models#o1\n",
        "\n",
        "**Feature:**\n",
        "* API allow user to change `reasoning_effort` parameter to give the model guidance on how many reasoning tokens it should generate before creating a response to the prompt. Works only on o1 and o3-mini for now.\n",
        "* o1 pricing:\n",
        "    * ðŸ’° $7.5 / million input tokens (cached)\n",
        "    * ðŸ’° $15 / million input tokens\n",
        "    * ðŸ’° $60 / million output tokens\n",
        "* o1-mini/o3-mini pricing:\n",
        "    * ðŸ’° $0.55 / million input tokens (cached)\n",
        "    * ðŸ’° $1.1 / million input tokens\n",
        "    * ðŸ’° $4.4 / million output tokens\n",
        "\n",
        "**API Documentation:**\n",
        "* Official Documentation: https://platform.openai.com/docs/guides/reasoning\n",
        "\n"
      ],
      "metadata": {
        "id": "W89EH12On97N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the key from the file and set the environmental variable\n",
        "with open(\"openai\",\"r\") as f:\n",
        "  key=f.read().strip()\n",
        "os.environ[\"OPENAI_API_KEY\"] = key"
      ],
      "metadata": {
        "id": "jCTE2Xj5cVmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "openai_reason = ChatOpenAI(\n",
        "    model=\"o1-mini\",\n",
        "    #reasoning_effort=\"medium\" # not supported for o1-mini but supported for o1 and o3-mini\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": prompt},\n",
        "]\n",
        "\n",
        "openai_response = openai_reason.invoke(messages)"
      ],
      "metadata": {
        "id": "NwmEU7tYxB8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_response.usage_metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnfdRbRvyIcy",
        "outputId": "395d0a9b-0a9e-4834-a142-bf948502e44b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_tokens': 31828,\n",
              " 'output_tokens': 2889,\n",
              " 'total_tokens': 34717,\n",
              " 'input_token_details': {'audio': 0, 'cache_read': 0},\n",
              " 'output_token_details': {'audio': 0, 'reasoning': 832}}"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(openai_response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqnrHWyCyy3v",
        "outputId": "9252f47c-a236-4bd6-adef-4c7e37af6c63",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Course Title:**  \n",
            "**Harnessing Reasoning Large Language Models: DeepSeek R1 and OpenAI o1**\n",
            "\n",
            "**Duration:**  \n",
            "15 Minutes\n",
            "\n",
            "---\n",
            "\n",
            "### **Course Outline**\n",
            "\n",
            "1. **Introduction to Reasoning LLMs** _(2 minutes)_\n",
            "2. **Key Features and Advantages** _(3 minutes)_\n",
            "3. **Getting Started: Accessing via API** _(2 minutes)_\n",
            "4. **Hands-On Python Coding Examples** _(5 minutes)_\n",
            "5. **Best Practices and Considerations** _(2 minutes)_\n",
            "6. **Conclusion and Further Resources** _(1 minute)_\n",
            "\n",
            "---\n",
            "\n",
            "### **1. Introduction to Reasoning LLMs** _(2 minutes)_\n",
            "\n",
            "- **What are Reasoning LLMs?**\n",
            "  - Advanced large language models trained with reinforcement learning.\n",
            "  - Designed to perform complex reasoning tasks by generating a **Chain of Thought (CoT)** before delivering the final answer.\n",
            "\n",
            "- **Examples:**\n",
            "  - **DeepSeek R1:** Open-sourced reasoning model with robust performance across various benchmarks.\n",
            "  - **OpenAI o1:** Proprietary model known for its superior reasoning capabilities and versatility.\n",
            "\n",
            "- **Target Audience:**\n",
            "  - Technical professionals seeking to enhance their AI and machine learning skill sets.\n",
            "\n",
            "---\n",
            "\n",
            "### **2. Key Features and Advantages** _(3 minutes)_\n",
            "\n",
            "- **Chain-of-Thought Reasoning:**\n",
            "  - Models generate detailed internal reasoning steps.\n",
            "  - Enhances accuracy and reliability in responses.\n",
            "\n",
            "- **Applications:**\n",
            "  - **Complex Problem Solving:** Mathematical computations, logic puzzles.\n",
            "  - **Coding Assistance:** Code generation, debugging, and refactoring.\n",
            "  - **Scientific Reasoning:** Data analysis, hypothesis generation.\n",
            "  - **Agentic Workflows:** Multi-step planning and task automation.\n",
            "\n",
            "- **Model Variants:**\n",
            "  - **Smaller Models (e.g., o3-mini):**\n",
            "    - Faster response times.\n",
            "    - Cost-effective per token.\n",
            "  - **Larger Models (e.g., o1):**\n",
            "    - Enhanced performance on complex tasks.\n",
            "    - Better generalization across domains.\n",
            "\n",
            "---\n",
            "\n",
            "### **3. Getting Started: Accessing via API** _(2 minutes)_\n",
            "\n",
            "- **API Endpoint:**\n",
            "  - **Chat Completions:** Central endpoint to interact with reasoning models.\n",
            "\n",
            "- **Key Parameters:**\n",
            "  - **model:** Specify either `o3-mini` or `o1` based on your needs.\n",
            "  - **reasoning_effort:** Determines the depth of internal reasoning. Options:\n",
            "    - `low` â€“ Faster, economical token usage.\n",
            "    - `medium` â€“ Balance between speed and reasoning depth.\n",
            "    - `high` â€“ Extensive reasoning, more tokens.\n",
            "  - **max_completion_tokens:** Controls the maximum number of tokens generated (including reasoning tokens).\n",
            "\n",
            "- **Example Scenario:**\n",
            "  - Generating a Python script to solve a specific task.\n",
            "  - Refactoring existing code based on certain criteria.\n",
            "\n",
            "---\n",
            "\n",
            "### **4. Hands-On Python Coding Examples** _(5 minutes)_\n",
            "\n",
            "#### **Example 1: Generating a Bash Script**\n",
            "\n",
            "**Objective:**  \n",
            "Write a bash script that takes a matrix string `\"[1,2],[3,4],[5,6]\"` and prints its transpose.\n",
            "\n",
            "```python\n",
            "import openai\n",
            "\n",
            "# Initialize OpenAI client\n",
            "openai.api_key = 'YOUR_API_KEY'  # Replace with your actual API key\n",
            "\n",
            "# Define the prompt\n",
            "prompt = \"\"\"\n",
            "Write a bash script that takes a matrix represented as a string with \n",
            "format '[1,2],[3,4],[5,6]' and prints the transpose in the same format.\n",
            "\"\"\"\n",
            "\n",
            "# Make the API call\n",
            "response = openai.ChatCompletion.create(\n",
            "    model=\"o3-mini\",\n",
            "    reasoning_effort=\"medium\",\n",
            "    messages=[\n",
            "        {\n",
            "            \"role\": \"user\",\n",
            "            \"content\": prompt\n",
            "        }\n",
            "    ],\n",
            "    max_completion_tokens=150\n",
            ")\n",
            "\n",
            "# Output the generated script\n",
            "script = response.choices[0].message.content\n",
            "print(script)\n",
            "```\n",
            "\n",
            "**Explanation:**\n",
            "- **Model Selection:** Uses `o3-mini` for cost-effectiveness.\n",
            "- **Reasoning Effort:** Set to `medium` for balanced performance.\n",
            "- **max_completion_tokens:** Limits the response to 150 tokens to manage costs.\n",
            "\n",
            "#### **Example 2: Refactoring a React Component**\n",
            "\n",
            "**Objective:**  \n",
            "Refactor a React component so that non-fiction books have red text.\n",
            "\n",
            "```python\n",
            "import openai\n",
            "\n",
            "# Initialize OpenAI client\n",
            "openai.api_key = 'YOUR_API_KEY'  # Replace with your actual API key\n",
            "\n",
            "# Define the prompt\n",
            "prompt = \"\"\"\n",
            "Instructions:\n",
            "- Given the React component below, change it so that nonfiction books have red\n",
            "  text. \n",
            "- Return only the code in your reply\n",
            "- Do not include any additional formatting, such as markdown code blocks\n",
            "- For formatting, use four space tabs, and do not allow any lines of code to \n",
            "  exceed 80 columns\n",
            "\n",
            "const books = [\n",
            "  { title: 'Dune', category: 'fiction', id: 1 },\n",
            "  { title: 'Frankenstein', category: 'fiction', id: 2 },\n",
            "  { title: 'Moneyball', category: 'nonfiction', id: 3 },\n",
            "];\n",
            "\n",
            "export default function BookList() {\n",
            "  const listItems = books.map(book =>\n",
            "    <li>\n",
            "      {book.title}\n",
            "    </li>\n",
            "  );\n",
            "\n",
            "  return (\n",
            "    <ul>{listItems}</ul>\n",
            "  );\n",
            "}\n",
            "\"\"\"\n",
            "\n",
            "# Make the API call\n",
            "response = openai.ChatCompletion.create(\n",
            "    model=\"o3-mini\",\n",
            "    reasoning_effort=\"high\",\n",
            "    messages=[\n",
            "        {\n",
            "            \"role\": \"user\",\n",
            "            \"content\": prompt\n",
            "        }\n",
            "    ],\n",
            "    max_completion_tokens=300\n",
            ")\n",
            "\n",
            "# Output the refactored component\n",
            "refactored_code = response.choices[0].message.content\n",
            "print(refactored_code)\n",
            "```\n",
            "\n",
            "**Explanation:**\n",
            "- **Model Selection:** Uses `o3-mini`.\n",
            "- **Reasoning Effort:** Set to `high` for detailed refactoring.\n",
            "- **max_completion_tokens:** Allows up to 300 tokens for comprehensive code.\n",
            "\n",
            "---\n",
            "\n",
            "### **5. Best Practices and Considerations** _(2 minutes)_\n",
            "\n",
            "- **Prompt Engineering:**\n",
            "  - **High-Level Guidance:** Reasoning models excel with broad instructions.\n",
            "  - **Less Micromanagement:** Trust the model to work out details.\n",
            "\n",
            "- **Managing Costs:**\n",
            "  - **Max Completion Tokens:** Limit to control expenses.\n",
            "  - **Reasoning Effort:** Choose appropriate levels based on task complexity.\n",
            "\n",
            "- **Context Window Management:**\n",
            "  - **Buffer for Reasoning Tokens:** Ensure enough tokens are reserved for internal reasoning.\n",
            "  - **Optimal Token Allocation:** Balance between input, reasoning, and output tokens.\n",
            "\n",
            "- **Testing and Validation:**\n",
            "  - **Iterative Testing:** Run multiple prompts to understand model behavior.\n",
            "  - **Error Handling:** Implement checks for incomplete or cutoff responses.\n",
            "\n",
            "---\n",
            "\n",
            "### **6. Conclusion and Further Resources** _(1 minute)_\n",
            "\n",
            "- **Summary:**\n",
            "  - Reasoning LLMs like DeepSeek R1 and OpenAI o1 offer powerful tools for complex tasks.\n",
            "  - Leveraging their Chain-of-Thought enhances accuracy and problem-solving capabilities.\n",
            "  - Proper parameter management ensures cost-effectiveness and optimal performance.\n",
            "\n",
            "- **Further Learning:**\n",
            "  - **Documentation:**\n",
            "    - [OpenAI API Reference](https://platform.openai.com/docs/api-reference/chat/create)\n",
            "    - [DeepSeek Documentation](https://github.com/deepseek-ai/DeepSeek-V3)\n",
            "  - **Cookbook Examples:**\n",
            "    - [Using Reasoning for Data Validation](https://cookbook.openai.com/examples/o1/using_reasoning_for_data_validation)\n",
            "    - [Using Reasoning for Routine Generation](https://cookbook.openai.com/examples/o1/using_reasoning_for_routine_generation)\n",
            "  - **Hands-On Practice:**\n",
            "    - Experiment with different prompts and parameters.\n",
            "    - Explore various use cases to harness the full potential of reasoning models.\n",
            "\n",
            "---\n",
            "\n",
            "### **Additional Python Code Example: Handling Reasoning Effort**\n",
            "\n",
            "**Objective:**  \n",
            "Demonstrate how adjusting `reasoning_effort` impacts the response.\n",
            "\n",
            "```python\n",
            "import openai\n",
            "\n",
            "# Initialize OpenAI client\n",
            "openai.api_key = 'YOUR_API_KEY'  # Replace with your actual API key\n",
            "\n",
            "# Define a complex problem prompt\n",
            "prompt = \"\"\"\n",
            "Explain the process of photosynthesis in detail and its importance to the ecosystem.\n",
            "\"\"\"\n",
            "\n",
            "# Function to get response based on reasoning effort\n",
            "def get_response(reasoning_effort_level):\n",
            "    response = openai.ChatCompletion.create(\n",
            "        model=\"o1\",\n",
            "        reasoning_effort=reasoning_effort_level,\n",
            "        messages=[\n",
            "            {\n",
            "                \"role\": \"user\",\n",
            "                \"content\": prompt\n",
            "            }\n",
            "        ],\n",
            "        max_completion_tokens=500\n",
            "    )\n",
            "    return response.choices[0].message.content\n",
            "\n",
            "# Low reasoning effort\n",
            "low_response = get_response(\"low\")\n",
            "print(\"Low Reasoning Effort Response:\\n\", low_response)\n",
            "\n",
            "# Medium reasoning effort\n",
            "medium_response = get_response(\"medium\")\n",
            "print(\"\\nMedium Reasoning Effort Response:\\n\", medium_response)\n",
            "\n",
            "# High reasoning effort\n",
            "high_response = get_response(\"high\")\n",
            "print(\"\\nHigh Reasoning Effort Response:\\n\", high_response)\n",
            "```\n",
            "\n",
            "**Explanation:**\n",
            "- **Purpose:** Illustrates how different `reasoning_effort` levels affect the depth and detail of the response.\n",
            "- **Usage:** Observe the variances in explanations based on effort settings.\n",
            "\n",
            "---\n",
            "\n",
            "### **Final Tips**\n",
            "\n",
            "- **Experimentation:** Utilize different models and parameters to find the best fit for your tasks.\n",
            "- **Resource Management:** Monitor token usage to balance between performance and cost.\n",
            "- **Stay Updated:** Follow updates from model developers to leverage the latest features and improvements.\n",
            "\n",
            "---\n",
            "\n",
            "**Thank you for attending this brief course on Reasoning LLMs!**  \n",
            "*For more in-depth learning, explore the provided resources and continue experimenting with the models to unlock their full potential.*\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenAI o1-mini Answer\n",
        "**Course Title:**  \n",
        "**Harnessing Reasoning Large Language Models: DeepSeek R1 and OpenAI o1**\n",
        "\n",
        "**Duration:**  \n",
        "15 Minutes\n",
        "\n",
        "---\n",
        "\n",
        "### **Course Outline**\n",
        "\n",
        "1. **Introduction to Reasoning LLMs** _(2 minutes)_\n",
        "2. **Key Features and Advantages** _(3 minutes)_\n",
        "3. **Getting Started: Accessing via API** _(2 minutes)_\n",
        "4. **Hands-On Python Coding Examples** _(5 minutes)_\n",
        "5. **Best Practices and Considerations** _(2 minutes)_\n",
        "6. **Conclusion and Further Resources** _(1 minute)_\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Introduction to Reasoning LLMs** _(2 minutes)_\n",
        "\n",
        "- **What are Reasoning LLMs?**\n",
        "  - Advanced large language models trained with reinforcement learning.\n",
        "  - Designed to perform complex reasoning tasks by generating a **Chain of Thought (CoT)** before delivering the final answer.\n",
        "\n",
        "- **Examples:**\n",
        "  - **DeepSeek R1:** Open-sourced reasoning model with robust performance across various benchmarks.\n",
        "  - **OpenAI o1:** Proprietary model known for its superior reasoning capabilities and versatility.\n",
        "\n",
        "- **Target Audience:**\n",
        "  - Technical professionals seeking to enhance their AI and machine learning skill sets.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Key Features and Advantages** _(3 minutes)_\n",
        "\n",
        "- **Chain-of-Thought Reasoning:**\n",
        "  - Models generate detailed internal reasoning steps.\n",
        "  - Enhances accuracy and reliability in responses.\n",
        "\n",
        "- **Applications:**\n",
        "  - **Complex Problem Solving:** Mathematical computations, logic puzzles.\n",
        "  - **Coding Assistance:** Code generation, debugging, and refactoring.\n",
        "  - **Scientific Reasoning:** Data analysis, hypothesis generation.\n",
        "  - **Agentic Workflows:** Multi-step planning and task automation.\n",
        "\n",
        "- **Model Variants:**\n",
        "  - **Smaller Models (e.g., o3-mini):**\n",
        "    - Faster response times.\n",
        "    - Cost-effective per token.\n",
        "  - **Larger Models (e.g., o1):**\n",
        "    - Enhanced performance on complex tasks.\n",
        "    - Better generalization across domains.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Getting Started: Accessing via API** _(2 minutes)_\n",
        "\n",
        "- **API Endpoint:**\n",
        "  - **Chat Completions:** Central endpoint to interact with reasoning models.\n",
        "\n",
        "- **Key Parameters:**\n",
        "  - **model:** Specify either `o3-mini` or `o1` based on your needs.\n",
        "  - **reasoning_effort:** Determines the depth of internal reasoning. Options:\n",
        "    - `low` â€“ Faster, economical token usage.\n",
        "    - `medium` â€“ Balance between speed and reasoning depth.\n",
        "    - `high` â€“ Extensive reasoning, more tokens.\n",
        "  - **max_completion_tokens:** Controls the maximum number of tokens generated (including reasoning tokens).\n",
        "\n",
        "- **Example Scenario:**\n",
        "  - Generating a Python script to solve a specific task.\n",
        "  - Refactoring existing code based on certain criteria.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Hands-On Python Coding Examples** _(5 minutes)_\n",
        "\n",
        "#### **Example 1: Generating a Bash Script**\n",
        "\n",
        "**Objective:**  \n",
        "Write a bash script that takes a matrix string `\"[1,2],[3,4],[5,6]\"` and prints its transpose.\n",
        "\n",
        "```python\n",
        "import openai\n",
        "\n",
        "# Initialize OpenAI client\n",
        "openai.api_key = 'YOUR_API_KEY'  # Replace with your actual API key\n",
        "\n",
        "# Define the prompt\n",
        "prompt = \"\"\"\n",
        "Write a bash script that takes a matrix represented as a string with\n",
        "format '[1,2],[3,4],[5,6]' and prints the transpose in the same format.\n",
        "\"\"\"\n",
        "\n",
        "# Make the API call\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"o3-mini\",\n",
        "    reasoning_effort=\"medium\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt\n",
        "        }\n",
        "    ],\n",
        "    max_completion_tokens=150\n",
        ")\n",
        "\n",
        "# Output the generated script\n",
        "script = response.choices[0].message.content\n",
        "print(script)\n",
        "```\n",
        "\n",
        "**Explanation:**\n",
        "- **Model Selection:** Uses `o3-mini` for cost-effectiveness.\n",
        "- **Reasoning Effort:** Set to `medium` for balanced performance.\n",
        "- **max_completion_tokens:** Limits the response to 150 tokens to manage costs.\n",
        "\n",
        "#### **Example 2: Refactoring a React Component**\n",
        "\n",
        "**Objective:**  \n",
        "Refactor a React component so that non-fiction books have red text.\n",
        "\n",
        "```python\n",
        "import openai\n",
        "\n",
        "# Initialize OpenAI client\n",
        "openai.api_key = 'YOUR_API_KEY'  # Replace with your actual API key\n",
        "\n",
        "# Define the prompt\n",
        "prompt = \"\"\"\n",
        "Instructions:\n",
        "- Given the React component below, change it so that nonfiction books have red\n",
        "  text.\n",
        "- Return only the code in your reply\n",
        "- Do not include any additional formatting, such as markdown code blocks\n",
        "- For formatting, use four space tabs, and do not allow any lines of code to\n",
        "  exceed 80 columns\n",
        "\n",
        "const books = [\n",
        "  { title: 'Dune', category: 'fiction', id: 1 },\n",
        "  { title: 'Frankenstein', category: 'fiction', id: 2 },\n",
        "  { title: 'Moneyball', category: 'nonfiction', id: 3 },\n",
        "];\n",
        "\n",
        "export default function BookList() {\n",
        "  const listItems = books.map(book =>\n",
        "    <li>\n",
        "      {book.title}\n",
        "    </li>\n",
        "  );\n",
        "\n",
        "  return (\n",
        "    <ul>{listItems}</ul>\n",
        "  );\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Make the API call\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"o3-mini\",\n",
        "    reasoning_effort=\"high\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt\n",
        "        }\n",
        "    ],\n",
        "    max_completion_tokens=300\n",
        ")\n",
        "\n",
        "# Output the refactored component\n",
        "refactored_code = response.choices[0].message.content\n",
        "print(refactored_code)\n",
        "```\n",
        "\n",
        "**Explanation:**\n",
        "- **Model Selection:** Uses `o3-mini`.\n",
        "- **Reasoning Effort:** Set to `high` for detailed refactoring.\n",
        "- **max_completion_tokens:** Allows up to 300 tokens for comprehensive code.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Best Practices and Considerations** _(2 minutes)_\n",
        "\n",
        "- **Prompt Engineering:**\n",
        "  - **High-Level Guidance:** Reasoning models excel with broad instructions.\n",
        "  - **Less Micromanagement:** Trust the model to work out details.\n",
        "\n",
        "- **Managing Costs:**\n",
        "  - **Max Completion Tokens:** Limit to control expenses.\n",
        "  - **Reasoning Effort:** Choose appropriate levels based on task complexity.\n",
        "\n",
        "- **Context Window Management:**\n",
        "  - **Buffer for Reasoning Tokens:** Ensure enough tokens are reserved for internal reasoning.\n",
        "  - **Optimal Token Allocation:** Balance between input, reasoning, and output tokens.\n",
        "\n",
        "- **Testing and Validation:**\n",
        "  - **Iterative Testing:** Run multiple prompts to understand model behavior.\n",
        "  - **Error Handling:** Implement checks for incomplete or cutoff responses.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Conclusion and Further Resources** _(1 minute)_\n",
        "\n",
        "- **Summary:**\n",
        "  - Reasoning LLMs like DeepSeek R1 and OpenAI o1 offer powerful tools for complex tasks.\n",
        "  - Leveraging their Chain-of-Thought enhances accuracy and problem-solving capabilities.\n",
        "  - Proper parameter management ensures cost-effectiveness and optimal performance.\n",
        "\n",
        "- **Further Learning:**\n",
        "  - **Documentation:**\n",
        "    - [OpenAI API Reference](https://platform.openai.com/docs/api-reference/chat/create)\n",
        "    - [DeepSeek Documentation](https://github.com/deepseek-ai/DeepSeek-V3)\n",
        "  - **Cookbook Examples:**\n",
        "    - [Using Reasoning for Data Validation](https://cookbook.openai.com/examples/o1/using_reasoning_for_data_validation)\n",
        "    - [Using Reasoning for Routine Generation](https://cookbook.openai.com/examples/o1/using_reasoning_for_routine_generation)\n",
        "  - **Hands-On Practice:**\n",
        "    - Experiment with different prompts and parameters.\n",
        "    - Explore various use cases to harness the full potential of reasoning models.\n",
        "\n",
        "---\n",
        "\n",
        "### **Additional Python Code Example: Handling Reasoning Effort**\n",
        "\n",
        "**Objective:**  \n",
        "Demonstrate how adjusting `reasoning_effort` impacts the response.\n",
        "\n",
        "```python\n",
        "import openai\n",
        "\n",
        "# Initialize OpenAI client\n",
        "openai.api_key = 'YOUR_API_KEY'  # Replace with your actual API key\n",
        "\n",
        "# Define a complex problem prompt\n",
        "prompt = \"\"\"\n",
        "Explain the process of photosynthesis in detail and its importance to the ecosystem.\n",
        "\"\"\"\n",
        "\n",
        "# Function to get response based on reasoning effort\n",
        "def get_response(reasoning_effort_level):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"o1\",\n",
        "        reasoning_effort=reasoning_effort_level,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            }\n",
        "        ],\n",
        "        max_completion_tokens=500\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Low reasoning effort\n",
        "low_response = get_response(\"low\")\n",
        "print(\"Low Reasoning Effort Response:\\n\", low_response)\n",
        "\n",
        "# Medium reasoning effort\n",
        "medium_response = get_response(\"medium\")\n",
        "print(\"\\nMedium Reasoning Effort Response:\\n\", medium_response)\n",
        "\n",
        "# High reasoning effort\n",
        "high_response = get_response(\"high\")\n",
        "print(\"\\nHigh Reasoning Effort Response:\\n\", high_response)\n",
        "```\n",
        "\n",
        "**Explanation:**\n",
        "- **Purpose:** Illustrates how different `reasoning_effort` levels affect the depth and detail of the response.\n",
        "- **Usage:** Observe the variances in explanations based on effort settings.\n",
        "\n",
        "---\n",
        "\n",
        "### **Final Tips**\n",
        "\n",
        "- **Experimentation:** Utilize different models and parameters to find the best fit for your tasks.\n",
        "- **Resource Management:** Monitor token usage to balance between performance and cost.\n",
        "- **Stay Updated:** Follow updates from model developers to leverage the latest features and improvements.\n",
        "\n",
        "---\n",
        "\n",
        "**Thank you for attending this brief course on Reasoning LLMs!**  \n",
        "*For more in-depth learning, explore the provided resources and continue experimenting with the models to unlock their full potential.*"
      ],
      "metadata": {
        "id": "DrmIPZdUAxY5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bGHPox2YDLMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare the reasoning models with a chat model like OpenAI GPT-4o or DeepSeek V3\n"
      ],
      "metadata": {
        "id": "GNRqVRqXKaet"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT-4o"
      ],
      "metadata": {
        "id": "ieJZ-CRPD8OM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai_chat = ChatOpenAI(\n",
        "    model=\"gpt-4o\"\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": prompt},\n",
        "]\n",
        "\n",
        "openai_chat_response = openai_chat.invoke(messages)"
      ],
      "metadata": {
        "id": "miGIpAm2DoW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_chat_response.usage_metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlSbf9eHEg_0",
        "outputId": "666cfd80-a05c-437d-ae11-f1e73cc6ca81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_tokens': 27043,\n",
              " 'output_tokens': 758,\n",
              " 'total_tokens': 27801,\n",
              " 'input_token_details': {'audio': 0, 'cache_read': 0},\n",
              " 'output_token_details': {'audio': 0, 'reasoning': 0}}"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(openai_chat_response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "x4DKUkmpElFz",
        "outputId": "d3454b55-1893-4696-8314-53e70fc0a0b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Certainly! Here's a 15-minute online course outline for technical professionals seeking to develop AI skills focused on reasoning LLMs such as DeepSeek R1 and OpenAI o1. This course will include Python coding examples.\n",
            "\n",
            "---\n",
            "\n",
            "### Course Title: Introduction to Reasoning Language Models: OpenAI o1 and DeepSeek R1\n",
            "\n",
            "#### Course Objective:\n",
            "Learn about reasoning language models and their applications in complex problem-solving, Scientific Research, and coding, with a practical understanding of using OpenAI o1 and DeepSeek R1.\n",
            "\n",
            "#### Target Audience:\n",
            "Technical professionals looking to enhance their AI and machine learning skills.\n",
            "\n",
            "---\n",
            "\n",
            "## Session Breakdown\n",
            "\n",
            "### **1. Introduction (2 minutes)**\n",
            "\n",
            "- Define Reasoning Language Models (RLMs).\n",
            "- Briefly introduce OpenAI o1 and DeepSeek R1.\n",
            "- Advantages of RLMs over traditional ML models.\n",
            "  \n",
            "### **2. How Reasoning Models Work (3 minutes)**\n",
            "\n",
            "- Explain the concept of Chain of Thought (CoT) and Reasoning Tokens.\n",
            "- Discuss the impact of `reasoning_effort` parameter in reasoning.\n",
            "\n",
            "#### **Visual Aid:**\n",
            "- Use diagrams to show how reasoning tokens are generated and discarded.\n",
            "  \n",
            "### **3. Using Reasoning Models in Python (6 minutes)**\n",
            "\n",
            "#### **3.1 Quickstart with OpenAI o1 (2 minutes)**\n",
            "\n",
            "- Explain how to interact with OpenAI o1 using Python.\n",
            "- Practical code demonstration on chat completion using OpenAI's API.\n",
            "\n",
            "**Python Example: Basic Chat Completion with o1**\n",
            "\n",
            "```python\n",
            "from openai import OpenAI\n",
            "client = OpenAI(api_key='<YOUR-API-KEY>')\n",
            "\n",
            "prompt = \"\"\"\n",
            "Explain the basics of reinforcement learning.\n",
            "\"\"\"\n",
            "\n",
            "response = client.chat.completions.create(\n",
            "    model=\"o1\",\n",
            "    reasoning_effort=\"medium\",\n",
            "    messages=[\n",
            "        {\n",
            "            \"role\": \"user\", \n",
            "            \"content\": prompt\n",
            "        }\n",
            "    ]\n",
            ")\n",
            "\n",
            "print(response.choices[0].message.content)\n",
            "```\n",
            "\n",
            "#### **3.2 Understanding DeepSeek R1 Models (2 minutes)**\n",
            "\n",
            "- Setup instructions for using DeepSeek R1.\n",
            "- Example of using DeepSeek API for Chain of Thought access.\n",
            "\n",
            "**Python Example: Accessing CoT with DeepSeek**\n",
            "\n",
            "```python\n",
            "from openai import OpenAI\n",
            "client = OpenAI(api_key='<DeepSeek-API-KEY>', base_url='https://api.deepseek.com')\n",
            "\n",
            "# Initial request\n",
            "messages = [{\"role\": \"user\", \"content\": \"Explain Euclidean geometry in simple terms.\"}]\n",
            "response = client.chat.completions.create(\n",
            "    model=\"deepseek-reasoner\",\n",
            "    messages=messages\n",
            ")\n",
            "\n",
            "reasoning_content = response.choices[0].message.reasoning_content\n",
            "content = response.choices[0].message.content\n",
            "\n",
            "print(\"Chain of Thought:\", reasoning_content)\n",
            "print(\"Final Response:\", content)\n",
            "```\n",
            "\n",
            "### **4. Best Practices and Use Cases (3 minutes)**\n",
            "\n",
            "- Discuss strategies for effective prompt engineering.\n",
            "- Explore practical applications such as coding assistance and scientific reasoning.\n",
            "\n",
            "#### **Resources and Examples:**\n",
            "- Provide links to OpenAI's best practices guides.\n",
            "- Mention real-world examples, such as creating multi-step plans or coding routines.\n",
            "\n",
            "### **5. Conclusion and Q&A (1 minute)**\n",
            "\n",
            "- Summarize the benefits of reasoning models.\n",
            "- Encourage experimentation and exploration beyond the course.\n",
            "- Open the floor for questions and additional resources suggestion.\n",
            "\n",
            "### **Further Reading and Resources**\n",
            "\n",
            "- Direct students to additional documentation on OpenAI and DeepSeek.\n",
            "- Provide links to Python libraries and examples used during the course.\n",
            "\n",
            "---\n",
            "\n",
            "**Note:**\n",
            "Ensure participants have the prerequisite API keys and installs required (e.g., `openai` package) before the course for an uninterrupted coding experience.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenAI GPT-4o Answer\n",
        "Certainly! Here's a 15-minute online course outline for technical professionals seeking to develop AI skills focused on reasoning LLMs such as DeepSeek R1 and OpenAI o1. This course will include Python coding examples.\n",
        "\n",
        "---\n",
        "\n",
        "### Course Title: Introduction to Reasoning Language Models: OpenAI o1 and DeepSeek R1\n",
        "\n",
        "#### Course Objective:\n",
        "Learn about reasoning language models and their applications in complex problem-solving, Scientific Research, and coding, with a practical understanding of using OpenAI o1 and DeepSeek R1.\n",
        "\n",
        "#### Target Audience:\n",
        "Technical professionals looking to enhance their AI and machine learning skills.\n",
        "\n",
        "---\n",
        "\n",
        "## Session Breakdown\n",
        "\n",
        "### **1. Introduction (2 minutes)**\n",
        "\n",
        "- Define Reasoning Language Models (RLMs).\n",
        "- Briefly introduce OpenAI o1 and DeepSeek R1.\n",
        "- Advantages of RLMs over traditional ML models.\n",
        "  \n",
        "### **2. How Reasoning Models Work (3 minutes)**\n",
        "\n",
        "- Explain the concept of Chain of Thought (CoT) and Reasoning Tokens.\n",
        "- Discuss the impact of `reasoning_effort` parameter in reasoning.\n",
        "\n",
        "#### **Visual Aid:**\n",
        "- Use diagrams to show how reasoning tokens are generated and discarded.\n",
        "  \n",
        "### **3. Using Reasoning Models in Python (6 minutes)**\n",
        "\n",
        "#### **3.1 Quickstart with OpenAI o1 (2 minutes)**\n",
        "\n",
        "- Explain how to interact with OpenAI o1 using Python.\n",
        "- Practical code demonstration on chat completion using OpenAI's API.\n",
        "\n",
        "**Python Example: Basic Chat Completion with o1**\n",
        "\n",
        "```python\n",
        "from openai import OpenAI\n",
        "client = OpenAI(api_key='<YOUR-API-KEY>')\n",
        "\n",
        "prompt = \"\"\"\n",
        "Explain the basics of reinforcement learning.\n",
        "\"\"\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"o1\",\n",
        "    reasoning_effort=\"medium\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n",
        "```\n",
        "\n",
        "#### **3.2 Understanding DeepSeek R1 Models (2 minutes)**\n",
        "\n",
        "- Setup instructions for using DeepSeek R1.\n",
        "- Example of using DeepSeek API for Chain of Thought access.\n",
        "\n",
        "**Python Example: Accessing CoT with DeepSeek**\n",
        "\n",
        "```python\n",
        "from openai import OpenAI\n",
        "client = OpenAI(api_key='<DeepSeek-API-KEY>', base_url='https://api.deepseek.com')\n",
        "\n",
        "# Initial request\n",
        "messages = [{\"role\": \"user\", \"content\": \"Explain Euclidean geometry in simple terms.\"}]\n",
        "response = client.chat.completions.create(\n",
        "    model=\"deepseek-reasoner\",\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "reasoning_content = response.choices[0].message.reasoning_content\n",
        "content = response.choices[0].message.content\n",
        "\n",
        "print(\"Chain of Thought:\", reasoning_content)\n",
        "print(\"Final Response:\", content)\n",
        "```\n",
        "\n",
        "### **4. Best Practices and Use Cases (3 minutes)**\n",
        "\n",
        "- Discuss strategies for effective prompt engineering.\n",
        "- Explore practical applications such as coding assistance and scientific reasoning.\n",
        "\n",
        "#### **Resources and Examples:**\n",
        "- Provide links to OpenAI's best practices guides.\n",
        "- Mention real-world examples, such as creating multi-step plans or coding routines.\n",
        "\n",
        "### **5. Conclusion and Q&A (1 minute)**\n",
        "\n",
        "- Summarize the benefits of reasoning models.\n",
        "- Encourage experimentation and exploration beyond the course.\n",
        "- Open the floor for questions and additional resources suggestion.\n",
        "\n",
        "### **Further Reading and Resources**\n",
        "\n",
        "- Direct students to additional documentation on OpenAI and DeepSeek.\n",
        "- Provide links to Python libraries and examples used during the course.\n",
        "\n",
        "---\n",
        "\n",
        "**Note:**\n",
        "Ensure participants have the prerequisite API keys and installs required (e.g., `openai` package) before the course for an uninterrupted coding experience."
      ],
      "metadata": {
        "id": "iSJlPiEfE4Gy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DeepSeek V3"
      ],
      "metadata": {
        "id": "F5YP22LDEw1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_deepseek import ChatDeepSeek\n",
        "\n",
        "deepseek_llm = ChatDeepSeek(\n",
        "    model=\"deepseek-chat\"\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": prompt},\n",
        "]\n",
        "\n",
        "deepseek_chat_response = deepseek_llm.invoke(messages)"
      ],
      "metadata": {
        "id": "G4bgRckMiNG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deepseek_chat_response.usage_metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiwWZqHSj-Od",
        "outputId": "85aaf81f-6de0-4562-f2cd-228e26348798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_tokens': 27090,\n",
              " 'output_tokens': 1319,\n",
              " 'total_tokens': 28409,\n",
              " 'input_token_details': {'cache_read': 0},\n",
              " 'output_token_details': {}}"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(deepseek_chat_response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jo0vfKhakWrX",
        "outputId": "8315ab16-55a1-4e25-9572-56babc790294",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### 15-Minute Online Course Plan: Reasoning with LLMs (DeepSeek R1 & OpenAI o1)\n",
            "\n",
            "**Target Audience:** Technical professionals seeking to develop AI skills.\n",
            "\n",
            "**Objective:** By the end of this course, participants will understand how reasoning models like DeepSeek R1 and OpenAI o1 work, how to use them for complex problem-solving, and how to implement basic coding examples in Python.\n",
            "\n",
            "---\n",
            "\n",
            "### **1. Introduction to Reasoning Models (2 minutes)**\n",
            "\n",
            "- **What are Reasoning Models?**\n",
            "  - Large Language Models (LLMs) like OpenAI o1 and DeepSeek R1 are trained using reinforcement learning to perform complex reasoning tasks.\n",
            "  - They generate an internal \"chain of thought\" before providing a response, making them excellent at tasks like coding, scientific reasoning, and multi-step planning.\n",
            "\n",
            "- **Key Features:**\n",
            "  - **Reasoning Tokens:** Models generate internal reasoning tokens to \"think\" before responding.\n",
            "  - **Reasoning Effort:** You can control the depth of reasoning using the `reasoning_effort` parameter (`low`, `medium`, `high`).\n",
            "\n",
            "---\n",
            "\n",
            "### **2. Quickstart: Using Reasoning Models (3 minutes)**\n",
            "\n",
            "- **API Endpoint:** Use the `chat completions` endpoint to interact with reasoning models.\n",
            "- **Python Example:**\n",
            "  ```python\n",
            "  from openai import OpenAI\n",
            "  client = OpenAI()\n",
            "\n",
            "  prompt = \"\"\"\n",
            "  Write a Python function to calculate the factorial of a number.\n",
            "  \"\"\"\n",
            "\n",
            "  response = client.chat.completions.create(\n",
            "      model=\"o3-mini\",\n",
            "      reasoning_effort=\"medium\",\n",
            "      messages=[\n",
            "          {\"role\": \"user\", \"content\": prompt}\n",
            "      ]\n",
            "  )\n",
            "\n",
            "  print(response.choices[0].message.content)\n",
            "  ```\n",
            "\n",
            "- **Explanation:**\n",
            "  - The `reasoning_effort` parameter controls how much the model \"thinks\" before responding.\n",
            "  - The model generates a Python function to calculate the factorial, demonstrating its ability to reason through coding tasks.\n",
            "\n",
            "---\n",
            "\n",
            "### **3. Managing Context and Costs (3 minutes)**\n",
            "\n",
            "- **Context Window:**\n",
            "  - Reasoning tokens occupy space in the model's context window.\n",
            "  - Ensure thereâ€™s enough space for reasoning tokens by setting `max_completion_tokens`.\n",
            "\n",
            "- **Cost Management:**\n",
            "  - Use `max_completion_tokens` to limit the total number of tokens generated (including reasoning tokens).\n",
            "  - Example:\n",
            "    ```python\n",
            "    response = client.chat.completions.create(\n",
            "        model=\"o3-mini\",\n",
            "        reasoning_effort=\"high\",\n",
            "        max_completion_tokens=500,\n",
            "        messages=[\n",
            "            {\"role\": \"user\", \"content\": prompt}\n",
            "        ]\n",
            "    )\n",
            "    ```\n",
            "\n",
            "---\n",
            "\n",
            "### **4. Prompting Best Practices (3 minutes)**\n",
            "\n",
            "- **High-Level Guidance:**\n",
            "  - Reasoning models perform best with high-level instructions rather than detailed step-by-step guidance.\n",
            "  - Example:\n",
            "    ```python\n",
            "    prompt = \"\"\"\n",
            "    Plan a Python project structure for a web scraper that saves data to a CSV file.\n",
            "    \"\"\"\n",
            "    ```\n",
            "\n",
            "- **Coding Example:**\n",
            "  - Ask the model to refactor code or implement algorithms.\n",
            "  - Example:\n",
            "    ```python\n",
            "    prompt = \"\"\"\n",
            "    Refactor the following Python code to use list comprehensions:\n",
            "    numbers = [1, 2, 3, 4, 5]\n",
            "    squares = []\n",
            "    for num in numbers:\n",
            "        squares.append(num ** 2)\n",
            "    \"\"\"\n",
            "    ```\n",
            "\n",
            "---\n",
            "\n",
            "### **5. DeepSeek R1: Chain of Thought (CoT) (2 minutes)**\n",
            "\n",
            "- **DeepSeek R1 Features:**\n",
            "  - Generates a **Chain of Thought (CoT)** before delivering the final answer.\n",
            "  - CoT is accessible via the API and can be used to understand the model's reasoning process.\n",
            "\n",
            "- **Python Example:**\n",
            "  ```python\n",
            "  from deepseek import DeepSeekClient\n",
            "  client = DeepSeekClient(api_key=\"your-api-key\")\n",
            "\n",
            "  response = client.chat.completions.create(\n",
            "      model=\"deepseek-reasoner\",\n",
            "      messages=[\n",
            "          {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem.\"}\n",
            "      ]\n",
            "  )\n",
            "\n",
            "  print(response.choices[0].message.reasoning_content)  # CoT\n",
            "  print(response.choices[0].message.content)  # Final Answer\n",
            "  ```\n",
            "\n",
            "---\n",
            "\n",
            "### **6. Practical Use Cases (2 minutes)**\n",
            "\n",
            "- **Coding:**\n",
            "  - Use reasoning models to generate, refactor, or debug code.\n",
            "  - Example: Ask the model to implement a sorting algorithm.\n",
            "\n",
            "- **STEM Research:**\n",
            "  - Use models to solve complex scientific problems or generate hypotheses.\n",
            "  - Example: Ask the model to explain a physics concept or derive a mathematical formula.\n",
            "\n",
            "- **Planning:**\n",
            "  - Use models to create multi-step plans for projects or workflows.\n",
            "  - Example: Ask the model to plan a machine learning pipeline.\n",
            "\n",
            "---\n",
            "\n",
            "### **7. Conclusion and Q&A (1 minute)**\n",
            "\n",
            "- **Key Takeaways:**\n",
            "  - Reasoning models like OpenAI o1 and DeepSeek R1 excel at complex problem-solving.\n",
            "  - Use `reasoning_effort` to control the depth of reasoning.\n",
            "  - Manage context and costs using `max_completion_tokens`.\n",
            "\n",
            "- **Q&A:**\n",
            "  - Open the floor for questions about using reasoning models in real-world applications.\n",
            "\n",
            "---\n",
            "\n",
            "### **Coding Examples Summary:**\n",
            "\n",
            "1. **Basic Python Function:**\n",
            "   ```python\n",
            "   prompt = \"Write a Python function to calculate the factorial of a number.\"\n",
            "   ```\n",
            "\n",
            "2. **Refactoring Code:**\n",
            "   ```python\n",
            "   prompt = \"Refactor the following Python code to use list comprehensions: ...\"\n",
            "   ```\n",
            "\n",
            "3. **Project Planning:**\n",
            "   ```python\n",
            "   prompt = \"Plan a Python project structure for a web scraper that saves data to a CSV file.\"\n",
            "   ```\n",
            "\n",
            "4. **DeepSeek CoT:**\n",
            "   ```python\n",
            "   response = client.chat.completions.create(\n",
            "       model=\"deepseek-reasoner\",\n",
            "       messages=[{\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem.\"}]\n",
            "   )\n",
            "   print(response.choices[0].message.reasoning_content)\n",
            "   ```\n",
            "\n",
            "---\n",
            "\n",
            "This course provides a concise yet comprehensive introduction to reasoning models, equipping technical professionals with the skills to leverage these advanced AI tools effectively.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DeepSeek V3 Answer\n",
        "### 15-Minute Online Course Plan: Reasoning with LLMs (DeepSeek R1 & OpenAI o1)\n",
        "\n",
        "**Target Audience:** Technical professionals seeking to develop AI skills.\n",
        "\n",
        "**Objective:** By the end of this course, participants will understand how reasoning models like DeepSeek R1 and OpenAI o1 work, how to use them for complex problem-solving, and how to implement basic coding examples in Python.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Introduction to Reasoning Models (2 minutes)**\n",
        "\n",
        "- **What are Reasoning Models?**\n",
        "  - Large Language Models (LLMs) like OpenAI o1 and DeepSeek R1 are trained using reinforcement learning to perform complex reasoning tasks.\n",
        "  - They generate an internal \"chain of thought\" before providing a response, making them excellent at tasks like coding, scientific reasoning, and multi-step planning.\n",
        "\n",
        "- **Key Features:**\n",
        "  - **Reasoning Tokens:** Models generate internal reasoning tokens to \"think\" before responding.\n",
        "  - **Reasoning Effort:** You can control the depth of reasoning using the `reasoning_effort` parameter (`low`, `medium`, `high`).\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Quickstart: Using Reasoning Models (3 minutes)**\n",
        "\n",
        "- **API Endpoint:** Use the `chat completions` endpoint to interact with reasoning models.\n",
        "- **Python Example:**\n",
        "  ```python\n",
        "  from openai import OpenAI\n",
        "  client = OpenAI()\n",
        "\n",
        "  prompt = \"\"\"\n",
        "  Write a Python function to calculate the factorial of a number.\n",
        "  \"\"\"\n",
        "\n",
        "  response = client.chat.completions.create(\n",
        "      model=\"o3-mini\",\n",
        "      reasoning_effort=\"medium\",\n",
        "      messages=[\n",
        "          {\"role\": \"user\", \"content\": prompt}\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  print(response.choices[0].message.content)\n",
        "  ```\n",
        "\n",
        "- **Explanation:**\n",
        "  - The `reasoning_effort` parameter controls how much the model \"thinks\" before responding.\n",
        "  - The model generates a Python function to calculate the factorial, demonstrating its ability to reason through coding tasks.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Managing Context and Costs (3 minutes)**\n",
        "\n",
        "- **Context Window:**\n",
        "  - Reasoning tokens occupy space in the model's context window.\n",
        "  - Ensure thereâ€™s enough space for reasoning tokens by setting `max_completion_tokens`.\n",
        "\n",
        "- **Cost Management:**\n",
        "  - Use `max_completion_tokens` to limit the total number of tokens generated (including reasoning tokens).\n",
        "  - Example:\n",
        "    ```python\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"o3-mini\",\n",
        "        reasoning_effort=\"high\",\n",
        "        max_completion_tokens=500,\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "    ```\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Prompting Best Practices (3 minutes)**\n",
        "\n",
        "- **High-Level Guidance:**\n",
        "  - Reasoning models perform best with high-level instructions rather than detailed step-by-step guidance.\n",
        "  - Example:\n",
        "    ```python\n",
        "    prompt = \"\"\"\n",
        "    Plan a Python project structure for a web scraper that saves data to a CSV file.\n",
        "    \"\"\"\n",
        "    ```\n",
        "\n",
        "- **Coding Example:**\n",
        "  - Ask the model to refactor code or implement algorithms.\n",
        "  - Example:\n",
        "    ```python\n",
        "    prompt = \"\"\"\n",
        "    Refactor the following Python code to use list comprehensions:\n",
        "    numbers = [1, 2, 3, 4, 5]\n",
        "    squares = []\n",
        "    for num in numbers:\n",
        "        squares.append(num ** 2)\n",
        "    \"\"\"\n",
        "    ```\n",
        "\n",
        "---\n",
        "\n",
        "### **5. DeepSeek R1: Chain of Thought (CoT) (2 minutes)**\n",
        "\n",
        "- **DeepSeek R1 Features:**\n",
        "  - Generates a **Chain of Thought (CoT)** before delivering the final answer.\n",
        "  - CoT is accessible via the API and can be used to understand the model's reasoning process.\n",
        "\n",
        "- **Python Example:**\n",
        "  ```python\n",
        "  from deepseek import DeepSeekClient\n",
        "  client = DeepSeekClient(api_key=\"your-api-key\")\n",
        "\n",
        "  response = client.chat.completions.create(\n",
        "      model=\"deepseek-reasoner\",\n",
        "      messages=[\n",
        "          {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem.\"}\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  print(response.choices[0].message.reasoning_content)  # CoT\n",
        "  print(response.choices[0].message.content)  # Final Answer\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Practical Use Cases (2 minutes)**\n",
        "\n",
        "- **Coding:**\n",
        "  - Use reasoning models to generate, refactor, or debug code.\n",
        "  - Example: Ask the model to implement a sorting algorithm.\n",
        "\n",
        "- **STEM Research:**\n",
        "  - Use models to solve complex scientific problems or generate hypotheses.\n",
        "  - Example: Ask the model to explain a physics concept or derive a mathematical formula.\n",
        "\n",
        "- **Planning:**\n",
        "  - Use models to create multi-step plans for projects or workflows.\n",
        "  - Example: Ask the model to plan a machine learning pipeline.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Conclusion and Q&A (1 minute)**\n",
        "\n",
        "- **Key Takeaways:**\n",
        "  - Reasoning models like OpenAI o1 and DeepSeek R1 excel at complex problem-solving.\n",
        "  - Use `reasoning_effort` to control the depth of reasoning.\n",
        "  - Manage context and costs using `max_completion_tokens`.\n",
        "\n",
        "- **Q&A:**\n",
        "  - Open the floor for questions about using reasoning models in real-world applications.\n",
        "\n",
        "---\n",
        "\n",
        "### **Coding Examples Summary:**\n",
        "\n",
        "1. **Basic Python Function:**\n",
        "   ```python\n",
        "   prompt = \"Write a Python function to calculate the factorial of a number.\"\n",
        "   ```\n",
        "\n",
        "2. **Refactoring Code:**\n",
        "   ```python\n",
        "   prompt = \"Refactor the following Python code to use list comprehensions: ...\"\n",
        "   ```\n",
        "\n",
        "3. **Project Planning:**\n",
        "   ```python\n",
        "   prompt = \"Plan a Python project structure for a web scraper that saves data to a CSV file.\"\n",
        "   ```\n",
        "\n",
        "4. **DeepSeek CoT:**\n",
        "   ```python\n",
        "   response = client.chat.completions.create(\n",
        "       model=\"deepseek-reasoner\",\n",
        "       messages=[{\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem.\"}]\n",
        "   )\n",
        "   print(response.choices[0].message.reasoning_content)\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        "This course provides a concise yet comprehensive introduction to reasoning models, equipping technical professionals with the skills to leverage these advanced AI tools effectively."
      ],
      "metadata": {
        "id": "mDWLggfmGZLI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tXDMpH6dGYRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reasoning models vs. Chat models\n",
        "Compared to chat models, reasoning models excel at different tasks and require different prompts.\n",
        "\n",
        "In general, reasoning models are more effective at strategizing, planning solutions to complex problems, and making decisions based on large volumes of ambiguous information. These models can also execute tasks with high accuracy and precision, making them ideal for domains that would otherwise require a human expertâ€”like math, science, engineering, financial services, and legal services.\n"
      ],
      "metadata": {
        "id": "t16eUhAXenfi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advice on prompting\n",
        "\n",
        "There are some differences to consider when prompting a reasoning model versus prompting a chat model. Generally speaking, reasoning models will provide better results on tasks with only high-level guidance. This differs somewhat from chat models, which often benefit from very precise instructions.\n",
        "\n",
        "* A reasoning model is like a senior co-worker - you can give them a goal to achieve, and trust them to work out the details.\n",
        "* A chat model is like a junior co-worker - they will perform best with explicit instructions to create a specific output.\n",
        "For more information on best practices when using reasoning models, refer to this guide.\n",
        "\n",
        "In [Wang et al. (2024)](https://arxiv.org/abs/2411.02093), researchers found for reasoning LLMs, the ability of sophisticated built-in reasoning reduces the impact of complex prompts, sometimes making simple zero-shot prompting more effective, especially in coding tasks."
      ],
      "metadata": {
        "id": "9RHvYL6j9uk_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Closing Remarks\n",
        "\n",
        "AI can't fully replace me yet so I'll be lazy and use the closing o1-mini wrote for me.\n",
        "\n",
        "---\n",
        "**Thank you for attending this brief course on Reasoning LLMs!**  \n",
        "*For more in-depth learning, explore the provided resources and continue experimenting with the models to unlock their full potential.*"
      ],
      "metadata": {
        "id": "eGAoesM1O7Uq"
      }
    }
  ]
}